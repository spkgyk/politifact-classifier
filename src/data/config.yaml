model_name: distilbert/distilbert-base-uncased-finetuned-sst-2-english
num_labels: 2
training_arguments:
  output_dir: output
  num_train_epochs: 2
  learning_rate: 0.00002
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 128
  weight_decay: 0.01
  evaluation_strategy: "epoch"