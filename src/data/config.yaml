model_name: distilbert/distilbert-base-uncased-finetuned-sst-2-english
num_labels: 2
training_arguments:
  output_dir: output
  num_train_epochs: 20
  learning_rate: 0.00001
  warmup_steps: 600
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  weight_decay: 0.01
  eval_strategy: epoch
  save_strategy: epoch
  load_best_model_at_end: true
  optim: adamw_torch
  group_by_length: true