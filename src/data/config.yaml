model_name: distilbert/distilbert-base-uncased-finetuned-sst-2-english
num_labels: 2
training_arguments:
  output_dir: output
  num_train_epochs: 20
  learning_rate: 0.00002
  per_device_train_batch_size: 128
  per_device_eval_batch_size: 128
  weight_decay: 0.01
  eval_strategy: "epoch"
  save_strategy: "epoch"
  load_best_model_at_end: true
  optim: adamw_torch
  group_by_length: true