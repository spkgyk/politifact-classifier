# model_name: FacebookAI/xlm-roberta-large
# model_name: google-bert/bert-base-uncased
model_name: distilbert/distilbert-base-uncased
num_labels: 2
inference_batch_size: 64
inference_model: output/distilbert/distilbert-base-uncased (GOOD)
inference_rf_model: output/random_forest.pkl
training_arguments:
  output_dir: output
  num_train_epochs: 10
  learning_rate: 0.00001
  warmup_steps: 600
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 32
  weight_decay: 0.01
  eval_strategy: epoch
  save_strategy: epoch
  load_best_model_at_end: true
  optim: adamw_torch
  group_by_length: true
  metric_for_best_model: accuracy
  greater_is_better: true
  save_total_limit : 3