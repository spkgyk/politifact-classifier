# model_name: FacebookAI/xlm-roberta-large
# model_name: google-bert/bert-base-uncased
model_name: distilbert/distilbert-base-uncased
num_labels: 2
inference_batch_size: 64
inference_model: output/distilbert/distilbert-base-uncased (GOOD)
training_arguments:
  output_dir: output
  num_train_epochs: 10
  learning_rate: 0.000005
  warmup_steps: 600
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 16
  weight_decay: 0.001
  eval_strategy: epoch
  save_strategy: epoch
  load_best_model_at_end: true
  optim: adamw_torch
  group_by_length: true
  metric_for_best_model: matthews_correlation
  greater_is_better: true
  save_total_limit : 3